---
title: "Data Science Project -  A Review of the Relationship Between Song Features and Its Relative Popularity With Respect to Time"
author: "Memes and Music: \nRich Gude \nSiwei Yang \nJunhe Zhang \nKrystal Payton"
date: "April 22, 2020"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: true
  pdf_document:
    toc: yes
---

<style type="text/css">
.main-container {
  
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = T, message = F)
# knitr::opts_chunk$set(warning = F, results = F, message = F)
# knitr::opts_chunk$set(include = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# 'scipen': integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than 'scipen' digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r basicfcn, include=F}
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
# note that using this function requires quotes around the package name, as you would when installing packages.
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
# unload/detact package when done using it
# detach_package = function(pkg, character.only = FALSE) { if(!character.only) { pkg <- deparse(substitute(pkg)) } search_item <- paste("package", pkg, sep = ":") while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } }
```

```{r outlierKD2, include = F}
# Fix outliers
outlierKD2 <- function(df, var, rm=FALSE) { 
    #' Original outlierKD functino by By Klodian Dhana,
    #' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
    #' Modified to have third argument for removing outliers instead of interactive prompt, 
    #' and after removing outlier, original df will not be changed. The function returns the new df, 
    #' which can be saved as original df name if desired.
    #' Check outliers, and option to remove them, save as a new dataframe. 
    #' @param df The dataframe.
    #' @param var The variable in the dataframe to be checked for outliers
    #' @param rm Boolean. Whether to remove outliers or not.
    #' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
    #' @examples
    #' outlierKD2(mydf, height, FALSE)
    #' mydf = outlierKD2(mydf, height, TRUE)
    #' mydfnew = outlierKD2(mydf, height, TRUE)
    dt = df # duplicate the dataframe for potential alteration
    var_name <- eval(substitute(var),eval(dt))
    na1 <- sum(is.na(var_name))
    m1 <- mean(var_name, na.rm = T)
    par(mfrow=c(2, 2), oma=c(0,0,3,0))
    boxplot(var_name, main="With outliers")
    hist(var_name, main="With outliers", xlab=NA, ylab=NA)
    outlier <- boxplot.stats(var_name)$out
    mo <- mean(outlier)
    var_name <- ifelse(var_name %in% outlier, NA, var_name)
    boxplot(var_name, main="Without outliers")
    hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
    
    # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
    # if(response == "y" | response == "yes"){
    if(rm){
        dt[as.character(substitute(var))] <- invisible(var_name)
        #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
        cat("Outliers successfully removed", "\n")
        return(invisible(dt))
    } else {
        cat("Nothing changed", "\n")
        return(invisible(df))
    }
}

```

# Executive Summary:



# Purpose:

The purpose of this study it to analyse the affect of music attributes, such as tempo, key, mode, etc, on the popularity of songs with respect to time.  Specifically, the SMART question for this study is: Does the key, valence, tempo, or other musical attributes changes between popular songs, and not popular songs for contrast, in the 1960's versus songs from the early 2000's?

The methodology by which this will be accomplished is to analyze song attribute data collected from Spotify using their application programming interface (API).  This attribute data collects the following features for every song, or "track", under review: track, artist, key, mode, valence, tempo, "energy", "liveness", and "danceability".  The popularility of each song will be a binary attribute: A song will be considered popular if it appears in the Billboard Hot 100, a weekly top list of the popular tracks from each week measured by Billboard Magazine.  Billboard Magazine is the widely considered standard for song popularity and has existed since the 1960s through today and, so, represents a consistent standard for measuring song popularity through time.

# A Discussion of Basic Music Theory

Music, as an art, has a long and varied history depending on the era and geographical space in which an individual may be concerned.  While the purpose of this study is to identify the change of popular music, at least over a short period since what is 50 years to the entire timeline of the human species over which music in some form is sure to have been evident, the entire history of what music is composed and how it is written or expressed will not be discussed herein.  Instead, this study is focused on contemporary, American-produced songs, and so, certain objective measurements of song qualities, namely tempo, key, and mode, can be used to compare and contrast songs both across the musically spectrum, from say jazz to rock-and-roll, and through time.

Tempo, key, and mode are relatively basic concepts in music theory.  The beat is the basic unit of time in music, the pace at which the music pulses; it is, essentially, when a listener would tap their toe during a song.  **Tempo** is the speed or pace of a given song and is often measured beats per minute.  Certain genres of songs are defined by high tempos, like electric-dance music (EDM), but most genres vary considerably in their tempo.  Pitch is the audio frequency at which individual notes within the song are heard by the listener.  In general terms, the **key** of a song is average pitch of a song, or the pitch around which a song fluctuates; performing a song in a higher key means that all of the notes of a song are increased at a level commiserate with the change in key (a major key change from "C'" to "D", one pitch, would change the pitch of all notes in the song up pitch as well).  Along the same concept as key, **mode** is the interval at which pitches are expressed in a song and is expressed as either "minor" or "major".  The key and mode are often expressed together when describing the pitch qualities of a song, such as "F major" or "D minor".  From an audio perspective, "major" keys are more often associated with and evoke happy or bright melodies, whereas "minor" keys may sound melancholic.

The concepts discussed here are general introduction to the features that will be discussed as part of this study's dataset, and the format herein does not support audio examples, which would be necessary for any comprehensive understanding of music theory.  Additional discussion of tempo, key, and mode, with audio examples for the differences between keys and modes, can be found [here](https://www.youtube.com/watch?v=rgaTLrZGlk0).

# Data Selection:

As identified in the methodology statement, the data in this study is pulled from two corporate sources, Spotify Technology S.A. (Spotify) and the Billboard-Hollywood Media Group.  Spotify is a popular online streaming service for music, videos, and podcasts.  Spotify provides an "Audio Analysis" of a musical tracks that describes the structure of the tracks and its musical content, including tempo, key, and mode, discussed above, in addition to more sophisticated musical metrics relating multiple core concepts of music.  The advance metrics that will be tracked from Spotify Audio Analysis in this study are valence, "danceability", "energy", and "liveness"; these metrics are discussed further below.  The Billboard-Hollywood Media Group owns and produces the "Billboard" magazine.  This publication is famous for the Billboard Hot 100 list, a weekly-published list that identifies the most popular, American songs of the week, based on sales and digital downloads and streams.  The Billboard Hot 100 have been in publication since 1958 and establishes an objective standard for identifies popular songs for the purposes of this study.

Based on the source from which popular songs are determined, the conclusions of this study should only apply to songs produced or with a large-commerical release in the United States of America.  Song features for popular songs based on similar metrics of sales and digital views from other countries may vary from the results reported herein.

The data analyized herein was pulled from the Kaggle database website.  The specific dataset can be found [here](https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset).

# Data Background:

As stated previously, the Spotify Audio Analysis records multiple features for each track in its extensive collection of music titles.  Some of these features, such as tempo, key, and mode, have strict definitions within the music community and are, otherwise, clear, quantifiable variables (e.g., tempo is measured in beats per minute).  Other features do not have strict definitions within the music community and/or do not have a clear, quantifiable standard that can be measured within each track.  The following features are ranked in Spotify's analyses on a scale of 0 to 1 and will be considered for analysis in this study: [^1]
- **Valence** describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).  For perspective: from the 2000's data set, one popular song with a high valence (0.965) is OutKast's [Hey Ya](https://open.spotify.com/track/2PpruBYCo4H7WOBJ7Q2EwM?si=3b7dll0ITZ-H5FsIu-qGig), while a song with low valence (0.0356) is deadmau5's [Strobe](https://open.spotify.com/track/31NiyZrUd1r4icY7xkvnWv?si=przVh9EjRgG-oyH5rInY9A).
- **Danceability** describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. For perspective: from the 2000's data set, one popular song with a high danceability (0.956) is Nelly's [Hot in Herre](https://open.spotify.com/track/04KTF78FFg8sOHC1BADqbY?si=i8f_wIreTsu34rIpSX4PDA), while a song with low danceability (0.0356) is Venom's [Black Metal](https://open.spotify.com/track/3yNoEJifUJdly8ucYoWRwl?si=15rDImzOTkSfJlpggbrPLw). 
- **Energy** represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. For perspective: from the 2000's data set, one popular song with a high energy (0.991) is Fatboy Slim's [The Rockafeller Skank](https://open.spotify.com/track/7mCQK9YB25WZw1saUjfL4e?si=46Rwt-TMQgusLo_Wjm3o0g), while a song with low energy (0.0013) is Alvin Curran's [Inner Cities II](https://open.spotify.com/track/4De0j0rVNmezk0EXPzOtwZ?si=ShNPfVWrQfKzZPiBlxKS-g).
- **Liveness** detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. For perspective: from the 2000's data set, one popular song with a high liveness (0.959) is Metallica's [No Leaf CLover - Live](https://open.spotify.com/track/0n4AllHzf3ma4ki20Y9h00?si=d3-JKQO9SOeTVxMzNhOYYQ), while any studio-produced song would have a low liveness.

The full analysis of these features, namely the methods or calculations by which their values are determined, are not released by Spotify, potentially due to intellectual property reasons.

The Billboard Hot 100 has several component charts that contribute to the overall calculation of the Hot 100 each week. The most significant components are: [^2]
- **Hot 100 Airplay**: (per Billboard) approximately 1,000 stations, "composed of adult contemporary, R&B, hip hop, country, rock, gospel, Latin and Christian formats, digitally monitored twenty-four hours a day, seven days a week. Charts are ranked by number of gross audience impressions, computed by cross-referencing exact times of radio airplay with Arbitron listener data." 
- **Hot Singles Sales**: (per Billboard) "the top selling singles compiled from a national sample of retail store, mass merchant and internet sales reports collected, compiled, and provided by Nielsen SoundScan." The chart is released weekly and measures sales of physical commercial singles. With the decline in sales of physical singles in the US, many songs that become number one on this chart often do not even chart on the Hot 100.
- **Digital Songs**: Digital sales are tracked by Nielsen SoundScan and are included as part of a title's sales points.
- **Streaming Songs**: a collaboration between Billboard, Nielsen SoundScan and National Association of Recording Merchandisers which measures the top streamed radio songs, on-demand songs and videos on leading online music services.

From these varied and independent sources, the Billboard Hot 100 represents a nuetral and storied arbiter of the objective popularity of songs from 1958 thorugh the present, including all songs to be evaluated from the 1960s and early 2000s.

# Data Preprocessing

The Spotify Audio Analysis stores up to 42 music features for each track.  For the purposes of this study, only the following variables will be analyzed and reviewed:

``` {r DataImport, include = F, results = F}
loadPkg('tidyverse')

# Import 1960's and 2000's data as separate dataframes
data1960 <- subset(data.frame(read_csv('dataset-of-60s.csv')), select = c('track', 'artist', 'tempo', 'key', 'mode', 'valence', 'danceability', 'energy', 'liveness', 'target'))
data2000 <- subset(data.frame(read_csv('dataset-of-00s.csv')), select = c('track', 'artist', 'tempo', 'key', 'mode', 'valence', 'danceability', 'energy', 'liveness', 'target'))
# Provide a summary of the columns and data within the data
summary(data2000)
length((which(data2000$target == 1)))
nrow(subset(data2000,target == 0))
length(data1960$target[data1960$target == 1])
length(data1960$target[data1960$target == 0])
```

**Track Variables of Interest:**

1. Track: the name of the track
2. Artist: the name of the artist
3. Tempo: a float value for the beats per minute of the song 
4. Key: an integer mapping of the standard pitch-class notation where: 0 = C key, 1 = C*#*/D*b*, 2 = D, and so on in rising key fashion up to 11 = B key.
5. Mode: an integer mapping for major (1) and minor (0) keys
6. Valence: a float value between 0 and 1 for the relative valence of the track (discussed in the Data Background)
7. Danceabilty: a float value between 0 and 1 for the relative danceability of the track (discussed in the Data Background)
8. Energy: a float value between 0 and 1 for the relative energy of the track (discussed in the Data Background)
9. Liveness: a float value between 0 and 1 for the relative liveness of the track (discussed in the Data Background)

In preprocessing the data, all of the variables from the Spotify data not listed above were eliminated from the study dataset.

The final feature, **Target**, was computed from the collective data from Billboard's Hot 100 list.  Any track listed in the Billboard Hot 100 during the respective decade from which a track was released is given a value of 1, and any track not list at any point in the Billboard Hot 100 is given a 0.

This study will examine the change in music metrics affecting popularity with respect to time.  For this purpose, this study will analyze two datasets, one with songs released during the 1960s, containing `r nrow(data1960)` songs, and the other with songs released from the 2000s, containing `r nrow(data2000)` songs.  Each dataset is composed of an equal number of popular and not popular songs, `r nrow(subset(data2000,target == 1))` popular and not popular songs from the 2000s and `r nrow(subset(data1960,target == 1))` popular and not popular songs from the 1960s.  For every song that appears in the Billboard Hot 100 list from their respective decade (with a target value of 1), another song from the same decade, not appearing in the Hot 100 was chosen to fill in the dataset for analytics and review.

A summary of the variables and their values from the entire 2000's and 1960's datasets are presented below:

``` {r DataSummary1}
# Provide a summary of the columns and data within the data
print('A Summary of the 2000\'s Dataset:')
summary(data2000)

print('A Summary of the 1960\'s Dataset:')
summary(data1960)
```

# Variable Anaylsis and Approach

The purpose of this study is to identify any link or lackthereof between the popularity of a song and seven other factors, tempo, key, mode, valence, danceability, energy, and liveness, based on track metric data computed from Spotify.  A cursory review of the song feature values for just popular between the two decades shows a difference in the mean values for multiple song features.  From the 1960's to the early 2000's, the mean valence noticeably decreases while the mean danceability and energy values noticeably increase.

``` {r DataSummary2}
# Provide a summary of the columns and data within the data
print('A Summary of the Popular 2000\'s Tracks:')
summary(subset(data2000, target == 1))

print('A Summary of the Popular 1960\'s Tracks:')
summary(subset(data1960, target == 1))
```

# K-Nearest Neighbor - Juhne

The project goal is to explore which features determine whether a song is popular or not. K-Nearest Neighbor is a good option to apply on this dataset, because both of our 1960s and 2000s datasets have a lot of numerical features and the target we want to predict on is a catagorical variable which is a perfect situation to apply K-Nearest Neighbor algorithm. Since K-Nearest Neighbor algorithm use distance function to measure distance among each data point, make sure that every predictor has same unit is very important. 

From the previous EDA section it showed that **tempo, key and mode** have much greater mean value compare to other predictors. Therefore, in order to make the most accuary K-Nearest Neighbor model, scaling the data using Z-score before building K-Nearest Neighbor model is very necessary.

On the other hand, since the K-Nearest Neighbor algorithm highly depend on distance function. Cleaning the categorical features like **track and artist** is also very important.
 
From the scaled dataset, notice that **tempo** for 2000s data is smaller than 1960s data in unit factor, the **mode and valence** value range for 2000s data are more spread than 1960 data showing that musice in 2000 have more varieties than music in 1960. However, the 2000s' musics are in general have more **energy** than 1960's musics. 

## K-Nearest Neighbor Scaling

### scale (data 2000)

Here we apply simple standard scaling technic which is z-score scaling, that scale our numerical variables with their standard deviation so that all of the scaled numerical variables will have their mean equal to zero. This approach is suitable in most of the time when trying to build model that applies distance function like **Euclidean distance** or **Manhattan distance**.

```{r scale-data-junhe-20}
# scale tempo ~ liveness
df.scaled.20 <- as.data.frame(scale(data2000[3:9], center = TRUE, scale = TRUE))

df.scaled.20$target <- as.factor(data2000$target)

summary(df.scaled.20)
```

### scale (data 1960)

We apply z-score scaling on our 1960s dataset as well, so that two K-Nearest Neighbor models we build for both datasets in the future have the closest characteristic. Then, we can compare the features that involved in two models and try to discover the most important features that did not change through time as well as the features that have changed through time that represent the change of taste of music of the audience through 1960s to 2000s.

```{r scale-data-junhe-60}
# scale tempo ~ liveness
df.scaled.60 <- as.data.frame(scale(data1960[3:9], center = TRUE, scale = TRUE))

df.scaled.60$target <- as.factor(data1960$target)

summary(df.scaled.60)
```

## K-Nearest Neighbor Correlation Matrix

### KNN-correlation matrix of (data 2000)

Before building KNN model, in order to preventing high collinearity between each predictor, it is also necessary to check the correlation between each predictors. From the correlation matrix for 2000s dataset, all predictors are not highly related to each other.

```{r KNN-correlation-junhe-20}
loadPkg('psych')
#pairs(df.scaled.20)
pairs.panels(data2000, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB", # set histogram color, can use "#22AFBB", "red",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )

```

### KNN-correlation matrix of (data 1960)

Here we do the same correlation matrix as before for 1960s model and also looks good. It is time to start building our models.

```{r KNN-correlation-junhe-60}

#pairs(df.scaled.60)
pairs.panels(data1960, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB", # set histogram color, can use "#22AFBB", "red",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
#unloadPkg('psych')
```

## K-Nearest Neighbor Spliting data into train test subsets

### KNN split data into train test subsets (data 2000)

Since datas are very precious, it is always a good idea to split the original dataset into training and testing subsets. Use training data set to fit KNN model and use testing dataset to test the accuracy of our the model is the best way to utilize the dataset. Set seed to 42 so that the result is reproductable, spliting the original data into 4:1 training and testing subset is a very common ratio to use.

```{r train-test-junhe-20}
# split data into train test subset, ratio 4:1, seed 42
seed <- 42
## set the seed to make the partition reproducible
set.seed(seed)
## 80% of the sample size
smp_size <- floor(0.80 * nrow(data2000))
# scale tempo ~ liveness
# df.scaled.20 <- as.data.frame(scale(data2000[3:9], center = TRUE, scale = TRUE))
# df.scaled.20$target <- as.factor(data2000$target)
summary(df.scaled.20)

dat20_sample <- sample(2, nrow(df.scaled.20), replace=TRUE, prob=c(0.80, 0.20))

```

### KNN split data into train test subsets (data 1960)

We do the same spliting process again for our 1960s dataset to keep our entire model building process consistent and comparable. 

```{r train-test-junhe-60}
# split data into train test subset, ratio 4:1, seed 42
seed <- 42
## set the seed to make the partition reproducible
set.seed(seed)
## 80% of the sample size
smp_size <- floor(0.80 * nrow(data1960))
# scale tempo ~ liveness
# df.scaled.20 <- as.data.frame(scale(data2000[3:9], center = TRUE, scale = TRUE))
# df.scaled.20$target <- as.factor(data2000$target)
summary(df.scaled.60)

dat60_sample <- sample(2, nrow(df.scaled.60), replace=TRUE, prob=c(0.80, 0.20))

```

## K-Nearest Neighbor Seperate target variable

### KNN seperate target variable y and predictors X (data 2000)

In order to test accuracy of the fitted models, spliting target variable **target** (1 stands for popular, 0 stands for not popular) variable from the predictors is also necessary.

```{r y-X-junhe-20}
train.X.20 <- df.scaled.20[dat20_sample==1, 1:7]
train.y.20 <- df.scaled.20[dat20_sample==1, 8]
test.X.20 <- df.scaled.20[dat20_sample==2, 1:7]
test.y.20 <- df.scaled.20[dat20_sample==2, 8]
#dim(test.X.20)[1]/dim(df.scaled.20)[1]
summary(test.X.20)
dim(train.X.20)
```

### KNN seperate target variable y and predictors X (data 1960)

We do the same target separation step for 1960s dataset for later accuracy testing as same as the previous step we did for 2000s dataset.

```{r y-X-junhe-60}
train.X.60 <- df.scaled.60[dat60_sample==1, 1:7]
train.y.60 <- df.scaled.60[dat60_sample==1, 8]
test.X.60 <- df.scaled.60[dat60_sample==2, 1:7]
test.y.60 <- df.scaled.60[dat60_sample==2, 8]
#dim(test.X.60)[1]/dim(df.scaled.60)[1]
summary(test.X.60)
dim(train.X.60)
```

## K-Nearest Neighbor helper function to choose best k value

This helper function apply K-Nearest Neighbor model function from **class** library, then build confusion matrix using the predicted target value with the test target value. Finally, using the formula **TP/(TP+NP)** to calculate accuracy of the fitted KNN model with given K value. 

Accuracy is a very important factor to determine the goodness of a fitted KNN model. On the other hand, because of high k-value indicates high precision and high cost on computation, carefully determine the appropriate k-value in order to manage best the trade-off between precision and computation cost is very important. In other word, using the exhaust approach to test a series of k-values and find the most appropriate k-value is the best way to do.

```{r checkK-helper-junhe}
# load class package for knn
loadPkg('class')
# function to selection best k #
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}
```

## K-Nearest Neighbor helper function to selection best combination of features with highest accuracy

In order to build the best KNN model with the given datasets to predict whether a music is popular or not. It is necessary to use exhaust approach to build model with all combinations of predictors and decide which combination of predictors can generate the most accurate model.

The **featureSelectionKNN** helper function applies **combn** function which helps generate a list of **n Choose i** feature combinations, where n stands for number of predictors we have in our dataset in our case is **tempo, key, mode, valence, danceability, energy and liveness** total of 7 features, i stands for number of predictors we want to consider in each iteration from i to n in our case is 1 to 7. Within each iteration, apply **chooseK** function to determine the best k-value for this combination of predictors. Then store the model with the highest accuracy into the result dataframe.

The time complexity for this algorithm is **[(7 Choose 1) times (21-3) times O(knn_function)] +...+ [(7 Choose 7) times (21-3) times O(knn_function)]**. Iterating from **7 Choose 1 to 7 Choose 7**, which is approximately equal to O(n^3), a very time consuming but accurate solution.

```{r featureSelection-junhe}
#features <- combn(names(train.X.20),3)
#features[,1]
#head(train.X.20[,features[,1]])
featureSelectionKNN <- function(train.X, test.X, train.y, test.y){
  res <- data.frame(k = 0, accuracy = 0, features = '')
  for(i in 3:dim(train.X)[2]-1){
    features <- combn(names(train.X),i)
    for(j in 3:dim(features)[2]-1){
      knn_different_k = sapply(seq(3, 21, by = 2),  #<- set k to be odd number from 3 to 21
                         function(x) chooseK(x, 
                                             train_set = train.X[,features[,j]],
                                             val_set = test.X[,features[,j]],
                                             train_class = train.y,
                                             val_class = test.y))
      
      knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])
      
      new.df = data.frame(k = knn_different_k[order(-knn_different_k$accuracy),][1,1],
                          accuracy = knn_different_k[order(-knn_different_k$accuracy),][1,2],
                          features = paste(features[,j],collapse=" ")
                          )
      res = rbind(res, new.df)
    }
    #print(dim(features)[2])
  }
  res
}

```

## KNN feature selection (data 2000)

```{r feature-selection-junhe-20}
features.20 = featureSelectionKNN(train.X.20, test.X.20, train.y.20, test.y.20)
# head(test[order(-test$accuracy),])
f.20 <- data.frame(features.20[order(-features.20$accuracy),])
head(f.20)
```

From the above result, the best KNN model for 2000s data after feature selection has accuracy equal to 0.75 with k-value equal 15 and with 5 features which are **tempo, valence, danceability, energy and liveness**.

The alternative option is the second best KNN model with accuracy equal to 0.75 as same as the best model but with lower k-value which equals 13, and 6 instead of 5 features which are **tempo, mode, valence, danceability, energy and liveness**. However, high k-value can cause expensive computation cost and second best KNN model seems to be better than the best KNN model in the list, we still want to minimize the number of involved predictors to be able to explore the most important factors that decide whether a music is popular or not in 2000s.

## KNN feature selection (data 1960)

```{r feature-selection-junhe-60}
features.60 = featureSelectionKNN(train.X.60, test.X.60, train.y.60, test.y.60)
# head(test[order(-test$accuracy),])
f.60 <- data.frame(features.60[order(-features.60$accuracy),])
head(f.60)
```

From the above result, the best KNN model for 1960s data after feature selection has accuracy equal to 0.68 with k-value equal 19 and has only 4 features which are **mode, valence, energy and liveness**.

The alternative option is the second best KNN model with accuracy equal to 0.68 as same as the best model but with higher k-value which equals 21, and 5 instead of 4 features which are **key, mode, valence, danceability, energy**. This time the second best KNN model loses on both k-value and number of predictors. So, we choose the best model without doubt as our final KNN model for 1960s dataset.

In conclusion, comparing the best model for 1960s dataset and 2000s dataset, the important features to determine whether a music is popular has changed from **mode, valence, energy and liveness** in 1960s to **tempo, valence, danceability, energy and liveness** in 2000s. In other word, people in 1960s took **mode, valence, energy and liveness** these four music elements as the most important features to determine whether a music is good or not, and has changed in 2000s to **tempo, valence, danceability, energy and liveness** these five music elements. People still think **valence, energy and liveness** are important factors but no longer took **mode** as an important factor. Moreover, took **tempo and danceability** as two new important factors.

##Visualize k-values in the best KNN model (data 2000)

After spotting the best features combination and k-values, it is time for us to start building our KNN model using 2000s dataset with the best subset of features and check correctness of our above **featureSelectionKNN** result. Notice that the best k-value among range 1 through 81 is still 15 which is as same as the result we get from **featureSelectionKNN** function. On the other hand, k-value equal 15 is also the ankle value of the k-values series which also demonstrate that this is the best k-value we are looking for.

```{r KNN-junhe-20}
# features selected by features selection function, with the best accuracy
best_features.20 <- c('tempo', 'valence', 'danceability', 'energy', 'liveness')

knn_different_k = sapply(seq(1, 81, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = train.X.20[,best_features.20],
                                             val_set = test.X.20[,best_features.20],
                                             train_class = train.y.20,
                                             val_class = test.y.20))

# Reformat the results to graph the results.
#str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg('ggplot2')

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
#summary(pima$type)[2]/dim(pima)[1]
#head(knn_different_k[order(knn_different_k$accuracy),])
#as.data.frame(knn_different_k[order(-knn_different_k$accuracy),])[1,2]
```

##Visualize k-value trend in the best KNN model (data 1960)

Same way to build our KNN model using 1960s dataset with the best subset of features and check correctness our above **featureSelectionKNN** result. Notice that the best k-value among 1 through 81 is still 19 which is as same as the result we get from **featureSelectionKNN** function. On the other hand, k-value equal 19 is also the ankle value of the k-values series which also demonstrate that this is the best k-value we are looking for.

The correctness of all the results has now been proved

```{r KNN-junhe-60}
# features selected by features selection function, with the best accuracy
best_features.60 <- c('mode', 'valence', 'energy', 'liveness')

knn_different_k = sapply(seq(1, 81, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = train.X.60[,best_features.60],
                                             val_set = test.X.60[,best_features.60],
                                             train_class = train.y.60,
                                             val_class = test.y.60))

# Reformat the results to graph the results.
#str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg('ggplot2')

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
#summary(pima$type)[2]/dim(pima)[1]
#head(knn_different_k[order(knn_different_k$accuracy),])
#as.data.frame(knn_different_k[order(-knn_different_k$accuracy),])[1,2]
```


# Logit Regression - Siwei
(For each test, write down the following: What the test says/Why are we performing, the assumptions of test, the math behind (why I trust the results), and discussion of conclusion - **NAMELY HOW THE VALUES OR FEATURES OR PROBABILITIES CHANGE BETWEEN THE TWO DATASETS as that is our purpose for this study**. Another good method is for each of the tests below, conduct all of the indiviudal tests that were performed in each weekly Rstudio document for said test (e.g., logit - our point is to impress with the data discussion and fill out a 10-page report))

(Linear Regression but with non-linear modeling - our target is binary so this is good)

# Feature Selection/Model selection - Krystal
(For each test, write down the following: What the test says/Why are we performing, the assumptions of test, the math behind (why I trust the results), and discussion of conclusion - **NAMELY HOW THE VALUES OR FEATURES OR PROBABILITIES CHANGE BETWEEN THE TWO DATASETS as that is our purpose for this study**. Another good method is for each of the tests below, conduct all of the indiviudal tests that were performed in each weekly Rstudio document for said test (e.g., logit - our point is to impress with the data discussion and fill out a 10-page report))

Suggestion: Include an analysis and discussion of why certain variables are best (find and discuss computing time for instance)

# Feature Selection/Model selection - Krystal

For the section, we will look at the Feature Selection in Linear Models.  We will continue using the dataset for the 1960s and 2000s music songs.  We will first eliminate any variables that we believe are useless for this section.  We are removing the variables labeled track, artist, target, and mode.  We also are removing any data that has NA as a number.  Please see the following:









(For each test, write down the following: What the test says/Why are we performing, the assumptions of test, the math behind (why I trust the results), and discussion of conclusion - **NAMELY HOW THE VALUES OR FEATURES OR PROBABILITIES CHANGE BETWEEN THE TWO DATASETS as that is our purpose for this study**. Another good method is for each of the tests below, conduct all of the indiviudal tests that were performed in each weekly Rstudio document for said test (e.g., logit - our point is to impress with the data discussion and fill out a 10-page report))

Suggestion: Include an analysis and discussion of why certain variables are best (find and discuss computing time for instance)



```{r setup2, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = 'markup', message = F)
knitr::opts_chunk$set(warning = F, results = 'hide', message = F)
# knitr::opts_chunk$set(include = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r basic, include=F}
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
loadPkg = function(pkg, character.only = FALSE) { 
  if (!character.only) { pkg <- as.character(substitute(pkg)) }
  pkg <- ifelse(!character.only, as.character(substitute(pkg)) , pkg)  
  if (!require(pkg,character.only=T, quietly =T)) {  install.packages(substitute(pkg),dep=T); if(!require(pkg,character.only=T)) stop("Package not found") } 
}
loadPkg(knitr)
# unload/detact package when done using it
unloadPkg = function(pkg, character.only = FALSE) { 
  if(!character.only) { pkg <- as.character(substitute(pkg)) } 
  search_item <- paste("package", pkg,sep = ":") 
  while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } 
}
```

```{r uzscale_fcn}
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
  
  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
```

```{r xkablesummary}
loadPkg(xtable)
loadPkg(kableExtra)
loadPkg(stringi)
xkabledply = function(modelsmmrytable, title="Table", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display model summary. 
  #' wrapper for the base::summary function on model objects
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param modelsmmrytable This can be a generic table, a model object such as lm(), or the summary of a model object summary(lm()) 
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return HTML table for display
  #' @examples
  #' library("xtable")
  #' library("kableExtra")
  #' xkabledply( df, title="Table testing", pos="left", bso="hover" )
  modelsmmrytable %>%
    xtable() %>% 
    kable(caption = title, digits = digits) %>%
    kable_styling(bootstrap_options = bso, full_width = FALSE, position = pos)
}
xkablesummary = function(df, title="Table: Statistics summary.", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param df The dataframe.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table for display, or for knitr to process into other formats 
  #' @examples
  #' xkablesummary( faraway::ozone )
  #' xkablesummary( ISLR::Hitters, title="Five number summary", pos="left", bso="hover"  )
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
    apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  xkabledply(s, title=title, digits = digits, pos=pos, bso=bso )
}
xkablevif = function(model, title="VIFs of the model", digits = 3, pos="left", bso="striped") { 
  #' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param model The lm or compatible model object.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table of the VIFs for a model for display, or for knitr to process into other formats 
  #' @examples
  #' xkablevif( lm(Salary~Hits+RBI, data=ISLR::Hitters) )
  
  vifs = table( names(model$coefficients)[2:length(model$coefficients)] ) # remove intercept to set column names
  vifs[] = faraway::vif(model) # set the values
  xkabledply( vifs, title=title, digits = digits, pos=pos, bso=bso )
}
```


```{r Classification, include=F}
loadPkg(rpart) # Classification trees, rpart(formula, data=, method=,control=) 
```


# Decision Trees
The main goal of this section is to predict whether the target class of popular and non popular songs will have different variables that affect them in the 1960s and 2000s.  Will the songs have the same variables that matter most in both years or will they differ? 

## Grow the Tree
In this section, the study will be using the Categorical Decision Tree to show what makes a song popular in the 1960s verses the 2000s.  The variables of tempo, key, valence, danceability, energy, liveness, and mode are the independent variables and target will be used as the dependent variable.  First there had to be changings made to the variable target so it could be a factor variable.  Once this has been done, one can proceed to grow the decision tree by starting at the root and building up.  Our root will be target which has already been stated as the dependent variable:

```{r cleandata, include=F}
data1960$target <- as.factor(data1960$target)
str(data1960)
data2000$target <- as.factor(data2000$target)
str(data2000)
```

```{r KPTree1960, echo = T, fig.dim=c(6,4)}
set.seed(1)
data1960_DT <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data1960, method="class", control = list(maxdepth = 10) )
printcp(data1960_DT) # display the results 
plotcp(data1960_DT) # visualize cross-validation results 
summary(data1960_DT) # detailed summary of splits
# plot tree 
plot(data1960_DT, uniform=TRUE, main="Classification Tree for 1960 Songs")
text(data1960_DT, use.n=TRUE, all=TRUE, cex=.8)
```

```{r KPTree2000, echo = T, fig.dim=c(6,4)}
set.seed(1)
data2000_DT <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data2000, method="class", control = list(maxdepth = 10) )
printcp(data2000_DT) # display the results 
plotcp(data2000_DT) # visualize cross-validation results 
summary(data2000_DT) # detailed summary of splits
# plot tree 
plot(data2000_DT, uniform=TRUE, main="Classification Tree for 2000 Songs")
text(data2000_DT, use.n=TRUE, all=TRUE, cex=.8)
```


```{r Postscript, results=T}
# create attractive postcript plot of tree 
post(data1960_DT, file = "data1960.ps", title = "Classification Tree for 1960s Song List")
post(data2000_DT, file = "data2000.ps", title = "Classification Tree for 2000s Song List")
```



Here are the results. 

For the 1960s Song List:
From the decision tree, the first branching point has 4321 popular and 4321 unpopular songs from the data. The first split yields 2015 outcomes with energy levels < 0.27 and 6627 outcomes with energy levels >= 0.27 for the popularity results for the song list.  The energy level was showns as being the most important variable to determine whether the song was popular or not.  The energy level being under 0.27 represents that the song was unpopular and the energy level greater than 0.27 deems the song as popular.  The variable that follows after the energy level was the variance level.  There were only two variables that determined the popularity of the songs making the Billboard Top 100 list. For the song to be considered popular, the energy of the song has to be equal to or greater than 0.27.  A further split on the left node might not have been beneficial to separate the popular and unpopular songs list. The algorithm actually stops there, and predicts unpopular for all 2015 observations, giving 476/2015 incorrect predictions. 

Nonethelss, the lower branch continue to split, with  (67%) correct prediction on the last leave (node at end of branch) and 481/772 correct **non popular** prediction on the third leaf, while the last leaf predicted 3604/5905 **popular** correctly. Not too bad.

Overall, when the model predicts **unpopular**, accuracy is (1539+481)/(1539+476+481+291) = `r round( (1539+481)/(1539+476+481+291)*100,1 )`%. And when the model predicts **popular**, accuracy is (29+12+12)/(29+0+12+0+12+2) = `r round( (29+12+12)/(29+0+12+0+12+2)*100,1 )`%.

Now the 2000s Song List can be summarized:
The decision tree starts the first branching point with 2936 popular and 2936 unpopular songs from the data presented from Spotify. The first split yields 1319 outcomes with danceability levels < 0.396 and 4553 outcomes with danceability levels >= 0.396 for the popularity results for the song list.  The danceability levels has been identified as the most important variable to determine whether the song was popular or not in the 2000s.  Danceability levels being under 0.396 represents that the song was unpopular while the danceability levels being greater than 0.396 are deemed as the song being popular.  The variables that follows after the danceability level are the energy and then valence levels.  Unlike the 1960s, there are three variables that determined the popularity of the songs making the Billboard Top 100 list. For the song to be considered popular, the danceability of the song has to be equal to or greater than 0.396.  A further split on the left node might not have been beneficial just like in the 1960s decision tree to separate the popular and unpopular songs list. The algorithm actually stops there, and predicts unpopular for all 1319 observations, giving 157/2015 incorrect predictions. 

Nonethelss, the lower branch continue to split, with  (60%) correct prediction on the last leaf (node at end of branch) and 129/214 correct **non popular** prediction on the third leaf, while the last leaf predicted 609/976 **popular** correctly.

Overall, when the model predicts **unpopular**, accuracy is (1539+481)/(1539+476+481+291) = `r round( (1539+481)/(1539+476+481+291)*100,1 )`%. And when the model predicts **popular**, accuracy is (29+12+12)/(29+0+12+0+12+2) = `r round( (29+12+12)/(29+0+12+0+12+2)*100,1 )`%.

From the decision tables above we see that what made a song popular in the 1960s did not necessarily make a song popular in the 2000s.  They had one differing variable that made the difference.  In the 2000s, the most important variable for the song to make the Billboards Top 100 list was for the song to have danceability.  Songs like Hot in Herre by Nelly is a song that has the danceability that is talked about.  In the 1960s, a song needed to have energy as the most impoartant variable.  We can check our decision tables but creating confusion matrixes.

These perentages for both the 1960s and 2000s songs can represent by the calculations below that are being examined in confusion matrixes.


```{r KPConTab1960, results=T}
loadPkg(caret) 
cm_1960 = confusionMatrix( predict(data1960_DT, type = "class"), reference = data1960[, "target"] )
print('Overall: ')
cm_1960$overall
print('Class: ')
cm_1960$byClass
unloadPkg(caret)
```

```{r KPTable1960, results="asis"}
xkabledply(cm_1960$table, "confusion matrix")
```

```{r KPConTable2000, results=T}
loadPkg(caret) 
cm_2000 = confusionMatrix( predict(data2000_DT, type = "class"), reference = data2000[, "target"] )
print('Overall: ')
cm_2000$overall
print('Class: ')
cm_2000$byClass
unloadPkg(caret)
```


```{r KPTable2000, results="asis"}
xkabledply(cm_2000$table, "confusion matrix")
```
Now that the calculations are calculated in the confusion matrix, another method can be used and it uses the maxdepth with the results being put into a summary below.


1960s Song Max Depth Summary:
```{r KPmaxdepth1960}
loadPkg(rpart)
loadPkg(caret)
# create an empty dataframe to store the results from confusion matrices
confusionMatrixResultDf_1 = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )
for (deep in 2:6) {
  fit1960 <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data1960, method="class", control = list(maxdepth = deep) )
  # 
  cm_1 = confusionMatrix( predict(fit1960, type = "class"), reference = data1960[, "target"] ) # from caret library
  # 
  cmaccu_1 = cm_1$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt_1 = data.frame(Depth=deep, Accuracy = cmaccu_1, row.names = NULL ) # initialize a row of the metrics 
  cmt_1 = cbind( cmt_1, data.frame( t(cm_1960$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf_1 = rbind(confusionMatrixResultDf_1, cmt_1)
  # print("Other metrics : ")
}
unloadPkg(caret)
```

The summarized result is here:

```{r TreeClass, results="asis"}
xkabledply(confusionMatrixResultDf_1, "1960s Song List Classification Trees summary with varying MaxDepth")
```

2000s Song Max Depth Summary:
```{r KPmaxdepth2000}
loadPkg(rpart)
loadPkg(caret)
# create an empty dataframe to store the results from confusion matrices
confusionMatrixResultDf_2 = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )
for (deep in 2:6) {
  fit2000 <- rpart(target ~ tempo + key + valence + danceability + energy + liveness, data=data2000, method="class", control = list(maxdepth = deep) )
  # 
  cm_2 = confusionMatrix( predict(fit2000, type = "class"), reference = data2000[, "target"] ) # from caret library
  # 
  cmaccu_2 = cm_2$overall['Accuracy']
  # print( paste("Total Accuracy = ", cmaccu ) )
  # 
  cmt_2 = data.frame(Depth=deep, Accuracy = cmaccu_2, row.names = NULL ) # initialize a row of the metrics 
  cmt_2 = cbind( cmt_2, data.frame( t(cm_2$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  confusionMatrixResultDf_2 = rbind(confusionMatrixResultDf_2, cmt_2)
  # print("Other metrics : ")
}
unloadPkg(caret)
```

The summarized result is here:

```{r TreeClass2, results="asis"}
xkabledply(confusionMatrixResultDf_2, "2000s Song List Classification Trees summary with varying MaxDepth")
```


The following are two additional ways that we can plot out the categorical decision tree.  The additional methods are the 'rpart.plot' and 'fancy' plots to help read the tree easier.  Both the 1960s and 2000s songs have been placed into the models as follows:


1960s Song Fancy and Rpart Plots
```{r KPfancyplot_1960, results=T}
loadPkg("rpart.plot")
rpart.plot(data1960_DT)
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
fancyRpartPlot(data1960_DT)
```

2000s Song Fancy and Rpart Plots
```{r KPfancyplot_2000, results=T}
loadPkg("rpart.plot")
rpart.plot(data2000_DT)
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
fancyRpartPlot(data2000_DT)
```


## Prune the tree

We can now prune the decision trees to get the most important results to know what variables are most important to be put onto the Billboards Top 100 list.  The following are the pruning of the trees:

```{r KPprune1960, results=T}
#prune the tree 
PT_data1960_DT <- prune(data1960_DT, cp = data1960_DT$cptable[2,"CP"])
#pkyphosisfit <- prune(kyphosisfit, cp = kyphosisfit$cptable[which.min( kyphosisfit$cptable[,"xerror"] ),"CP"])
# plot the pruned tree 
fancyRpartPlot(PT_data1960_DT)
# For boring plot, use codes below instead
plot(PT_data1960_DT, uniform=TRUE, main="Pruned Classification Tree for 1960s Song List")
text(PT_data1960_DT, use.n=TRUE, all=TRUE, cex=.8)
```


```{r KPprune2000, results=T}
#prune the tree 
PT_data2000_DT <- prune(data2000_DT, cp = data2000_DT$cptable[2,"CP"])
#pkyphosisfit <- prune(kyphosisfit, cp = kyphosisfit$cptable[which.min( kyphosisfit$cptable[,"xerror"] ),"CP"])
# plot the pruned tree 
fancyRpartPlot(PT_data2000_DT)
# For boring plot, use codes below instead
plot(PT_data2000_DT, uniform=TRUE, main="Pruned Classification Tree for 2000s Song List")
text(PT_data2000_DT, use.n=TRUE, all=TRUE, cex=.8)
```

From the pruned trees, the results show different variables for each year on what classified them to get on the Billboards Top 100 List.  For the 1960s songs, the song needed to have an energy level greater than 0.27.  This is shown as the most independent variable that determines the popularity of a song.  In order for a song in the 2000s to make the list, the danceability matter more than energy.  The dancebility had to be greater than a level of 0.40.  In the two eras, there were differences in what was deemed as most important.  In the 1960s, a person needed to have the right energy being eluded from the music while in the 2000s the song had to be a danceable song.


# Summary of Results



---
[^1] Taken from [Spotify for Developers Web Services Page](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/)
[^2] From Molanphy, Chris (August 1, 2013). "How The Hot 100 Became America's Hit Barometer". Published by NPR.

















